# Randomforest Hyperparameter
# https://cran.r-project.org/web/packages/ranger/ranger.pdf
randomforest:
  # Dynamic uses a grid search for best hyperparameter
  # TRUE or FALSE
  dynamic: TRUE
  # Number of trees >0
  num.trees: 500
  # Features >0 & <=max(features)
  # Number Variables as candidates at each split
  mtry: 2
  # Mininum of nodes >0
  min.node.size: 5
  # Samples to use >0 & <=1.0
  sample.fraction: 0.7
  # Maximum depth of the trees, 0 or NULL ->unlimited
  # Regularization
  max.depth: 3
  # NULL to use Rs gernerated seed
  seed: 123

# Anomalie detectors ---------------------------------------------------------------------------------------------------

# Isolationforest Hyperparameter
# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html
isolationforest:
  # Number of trees >0
  n_estimators: 50
  # Samples to use >0 & <=1.0
  max_samples: auto
  # Contamination >0 & <1.0
  contamination: 0.0001
  # Max Features to use >0 & <=1.0
  max_features: 1.0
  # NULL or number
  random_state: 123

# kNN Hyperparameter
# https://pyod.readthedocs.io/en/latest/pyod.models.html?highlight=knn#pyod.models.knn.KNN
k_nearest_neigbhour:
  # Contamination >0 & <1.0
  contamination: 0.0001
  # Number of neighbors >0 & <= max(samples)
  n_neighbors: 20
  # Methode to calculate distance (largest, mean, median)
  method: mean
  # Algorithm to optimize neigbhour search (ball_tree, kd_tree, brute, auto)
  algorithm: ball_tree
  # Distance metrics from scipy or scikit
  # 'cityblock', 'cosine', 'euclidean', 'l1', 'l2', 'manhattan','braycurtis', 'canberra', 'chebyshev', 'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski', 'mahalanobis', 'matching', 'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule'
  metric: minkowski

# DAGMM Hyperparameter
# https://github.com/tnakae/DAGMM/blob/master/dagmm/dagmm.py
deep_autoencoding_gaussian_mixture_model:
  # Dynamic will uses a dynamic neuron count per layer
  # TRUE or False
  dynamic: TRUE
  # Neurons per Compression layer
  # Array of ints, min = 1, last number is not mirrored
  comp_hiddens: [ 24, 16, 8, 4, 2 ]
  # Activation function fitted to tensorflow
  comp_activation: tanh
  # Neurons per Estimation layer
  # Array of ints, min = 1, last number = number of mixture model components
  est_hiddens: [ 16, 8, 4 ]
  # https://www.tensorflow.org/api_docs/python/tf/keras/activations
  # "deserialize","elu","exponential","gelu","get","hard_sigmoid","linear","relu","selu","serialize","sigmoid","softmax","softplus","softsign","swish","tanh"
  est_activation: tanh
  # Dropout ratio, <1.0 or NULL=0 for not applied
  # regularization factor to minimize overfitting
  est_dropout_ratio: 0.25
  # Epochs, min=100
  # Times it iterates over the complete dataset
  epoch_size: 2500
  # Min=1, Max=max(samples)
  # After x Samples the parameters of the network will be updated
  minibatch_size: 512
  random_seed: 123
